# llm_utils.py - VERSÃƒO COM MEMÃ“RIA INTELIGENTE INTEGRADA

import os
import pandas as pd
import logging
import re
from datetime import datetime
from typing import Optional, Tuple
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# âœ¨ IMPORTA MEMÃ“RIA INTELIGENTE
from memory_module import MemoriaInteligente

load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('chatfiscal.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class LLMInteligente:
    """
    LLM com suporte a CSV, XML, PDF (individual e consolidado)
    ğŸ§  AGORA COM MEMÃ“RIA INTELIGENTE INTEGRADA
    """

    def __init__(self):
        try:
            api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
            if not api_key:
                logger.warning("âš ï¸ GOOGLE_API_KEY ou GEMINI_API_KEY nÃ£o encontrada")

            self.llm = ChatGoogleGenerativeAI(
                model="gemini-2.0-flash",
                google_api_key=api_key,
                temperature=0.3
            )
            
            # âœ¨ INICIALIZA MEMÃ“RIA INTELIGENTE
            self.memoria = MemoriaInteligente(persist_dir="memoria_chatfiscal")
            logger.info("âœ… LLM Gemini v2.0-flash + MemoriaInteligente inicializados")
            
        except Exception as e:
            logger.error(f"âŒ Erro ao inicializar LLM: {e}")
            raise

    # âœ… ADICIONE ESTES MÃ‰TODOS QUE ESTAVAM COM "..."
    
    def _criar_sistema_prompt(self, tipo_resposta):
        """Cria prompt do sistema baseado no tipo de resposta"""
        
        if tipo_resposta == "csv":
            return """VocÃª Ã© um analista fiscal especializado em notas fiscais eletrÃ´nicas.

IMPORTANTE: Responda APENAS o que foi perguntado de forma natural e direta.

Regras:
- Use APENAS os dados JSON fornecidos
- Responda de forma natural, sem mencionar "dados estruturados", "registros" ou "com base em"
- Cite valores especÃ­ficos dos dados JSON quando relevante
- Use terminologia fiscal profissional quando apropriado
- Se um dado nÃ£o estÃ¡ disponÃ­vel no JSON, informe de forma clara e direta
- Seja conciso e objetivo
- NUNCA use tags HTML na resposta
- Responda em portuguÃªs brasileiro
- Para perguntas sobre prestador/tomador/emitente: use os campos correspondentes do JSON"""

        elif tipo_resposta == "pdf":
            return """VocÃª Ã© um especialista em anÃ¡lise de documentos fiscais.

Regras:
- Responda com base APENAS no documento fornecido
- NÃ£o mencione "no documento" ou "segundo o PDF" - responda diretamente
- Cite pÃ¡ginas apenas se for relevante para localizaÃ§Ã£o
- Se a informaÃ§Ã£o nÃ£o estÃ¡ disponÃ­vel, informe claramente
- Use linguagem tÃ©cnica mas natural
- NUNCA use tags HTML na resposta
- Responda em portuguÃªs brasileiro"""

        elif tipo_resposta == "consolidada":
            return """VocÃª Ã© um analista fiscal avanÃ§ado com acesso a mÃºltiplas fontes.

Regras:
- Cruze informaÃ§Ãµes entre registros eletrÃ´nicos e documentos quando relevante
- Responda de forma natural, sem mencionar explicitamente as fontes em cada frase
- Cite a origem apenas se houver discrepÃ¢ncia ou for essencial
- Use terminologia fiscal profissional
- Seja conciso e direto
- NUNCA use tags HTML na resposta
- Responda em portuguÃªs brasileiro"""

        else:
            return "VocÃª Ã© um assistente fiscal profissional."

    def _formatar_nome_campo(self, campo):
        """Formata nome de campo tÃ©cnico para apresentaÃ§Ã£o profissional"""
        campo = campo.replace('prestador_', '').replace('tomador_', '').replace('emit_', '').replace('dest_', '')
        campo = campo.replace('ide_', '').replace('nfse_', '').replace('item_', '')
        campo = campo.replace('_', ' ')
        
        palavras_especiais = {
            'email': 'E-mail', 'telefone': 'Telefone', 'cnpj': 'CNPJ', 'cpf': 'CPF',
            'ie': 'IE', 'im': 'IM', 'cep': 'CEP', 'uf': 'UF', 'icms': 'ICMS',
            'iss': 'ISS', 'cfop': 'CFOP', 'cst': 'CST', 'csosn': 'CSOSN',
            'valor': 'Valor', 'total': 'Total', 'numero': 'NÃºmero', 'data': 'Data',
            'emissao': 'EmissÃ£o', 'contato': 'Contato', 'endereco': 'EndereÃ§o',
            'razao': 'RazÃ£o', 'social': 'Social', 'fantasia': 'Nome Fantasia',
            'codigo': 'CÃ³digo', 'municipio': 'MunicÃ­pio', 'complemento': 'Complemento',
            'bairro': 'Bairro', 'logradouro': 'Logradouro', 'nfe': 'NF-e',
            'nfse': 'NFS-e', 'pis': 'PIS', 'cofins': 'COFINS', 'ipi': 'IPI',
            'aliquota': 'AlÃ­quota', 'base': 'Base', 'calculo': 'CÃ¡lculo',
            'produto': 'Produto', 'servico': 'ServiÃ§o', 'descricao': 'DescriÃ§Ã£o',
            'quantidade': 'Quantidade', 'unitario': 'UnitÃ¡rio'
        }
        
        palavras = campo.lower().split()
        palavras_formatadas = []
        
        for palavra in palavras:
            if palavra in palavras_especiais:
                palavras_formatadas.append(palavras_especiais[palavra])
            else:
                palavras_formatadas.append(palavra.capitalize())
        
        return ' '.join(palavras_formatadas)

    def _eh_saudacao_pura(self, pergunta):
        """
        Detecta se Ã© APENAS uma saudaÃ§Ã£o vazia.
        Retorna True apenas para saudaÃ§Ãµes simples SEM pergunta.
        """
        saudacoes_simples = {
            'oi', 'olÃ¡', 'ola', 'hey', 'hi', 'e aÃ­', 'e ai',
            'bom dia', 'boa tarde', 'boa noite', 'boa madrugada'
        }
        
        pergunta_lower = pergunta.lower().strip()
        
        if pergunta_lower in saudacoes_simples:
            return True
        
        if '?' in pergunta:
            return False
        
        if len(pergunta.split()) > 5:
            return False
        
        return False

    def gerar_resposta_llm(self, pergunta, df=None, contexto_pdf=None, historico=None):
        """
        Gera resposta baseado no que estÃ¡ disponÃ­vel.
        ğŸ§  AGORA COM BUSCA SEMÃ‚NTICA AUTOMÃTICA
        """
        
        tem_csv = df is not None and not df.empty
        tem_pdf = contexto_pdf is not None and contexto_pdf.strip()
        
        if not tem_csv and not tem_pdf:
            return "âŒ Nenhum dado disponÃ­vel. Carregue um arquivo CSV/XML ou PDF para comeÃ§ar."
        
        if self._eh_saudacao_pura(pergunta):
            return """OlÃ¡! ğŸ‘‹ Sou o assistente fiscal do ChatFiscal.

**Estou pronto para analisar seus dados fiscais!**

VocÃª pode me fazer perguntas como:
- Quantas notas fiscais foram carregadas?
- Qual Ã© o valor total das operaÃ§Ãµes?
- Quem Ã© o emitente da NF-e?
- Qual a alÃ­quota de ISS aplicada?
- Liste todos os prestadores de serviÃ§o
- Quais sÃ£o os CFOPs utilizados?

**Como posso ajudar?**"""
        
        # âœ¨ BUSCA CONTEXTO RELEVANTE DA MEMÃ“RIA
        contexto_memoria = self.memoria.buscar_contexto_relevante(pergunta, k=3)
        
        if contexto_memoria:
            logger.info(f"ğŸ§  Contexto relevante encontrado na memÃ³ria")
            # Adiciona contexto da memÃ³ria ao contexto PDF
            if contexto_pdf:
                contexto_pdf += f"\n\nğŸ“š CONVERSAS ANTERIORES RELACIONADAS:\n{contexto_memoria}"
            else:
                contexto_pdf = f"ğŸ“š CONVERSAS ANTERIORES RELACIONADAS:\n{contexto_memoria}"
        
        if tem_csv and tem_pdf:
            tipo_resposta = "consolidada"
        elif tem_pdf:
            tipo_resposta = "pdf"
        else:
            tipo_resposta = "csv"

        logger.info(f"ğŸ“‹ Tipo de resposta: {tipo_resposta}")
        
        try:
            sistema_prompt = SystemMessagePromptTemplate.from_template(
                self._criar_sistema_prompt(tipo_resposta)
            )

            if tipo_resposta == "csv":
                resposta = self._responder_csv(pergunta, df, sistema_prompt)
            elif tipo_resposta == "pdf":
                resposta = self._responder_pdf(pergunta, contexto_pdf, sistema_prompt)
            elif tipo_resposta == "consolidada":
                resposta = self._responder_consolidado(pergunta, df, contexto_pdf, sistema_prompt)
            
            # âœ¨ SALVA CONVERSA NA MEMÃ“RIA AUTOMATICAMENTE
            self.memoria.salvar_contexto(
                pergunta=pergunta,
                resposta=resposta,
                metadados_extras={
                    "tipo_resposta": tipo_resposta,
                    "tem_csv": tem_csv,
                    "tem_pdf": tem_pdf
                }
            )
            logger.info("ğŸ’¾ Conversa salva na memÃ³ria inteligente")
            
            return resposta

        except Exception as e:
            logger.error(f"Erro ao gerar resposta: {e}")
            return f"Erro ao processar pergunta: {str(e)}"

    # âœ… ADICIONE OS MÃ‰TODOS _responder_*
    
    def _responder_csv(self, pergunta, df, sistema_prompt):
        """Responde usando APENAS dados estruturados - VERSÃƒO CORRIGIDA"""
        logger.info("ğŸ“Š Modo: Dados Estruturados")
        
        try:
            tem_nfe = any(col.startswith('emit_') for col in df.columns)
            tem_nfse = any(col.startswith('prestador_') for col in df.columns)
            
            dados_json = df.head(50).to_json(orient='records', force_ascii=False, indent=2)
            
            total_registros = len(df)
            resumo_colunas = ", ".join(df.columns.tolist())
            
            colunas_valor = [col for col in df.columns if 'valor' in col.lower() or 'total' in col.lower()]
            estatisticas = ""
            if colunas_valor:
                for col in colunas_valor:
                    if pd.api.types.is_numeric_dtype(df[col]):
                        try:
                            estatisticas += f"\n{col}: Soma={df[col].sum():.2f}, MÃ©dia={df[col].mean():.2f}, Min={df[col].min():.2f}, Max={df[col].max():.2f}"
                        except:
                            pass
            
            contexto_estrutural = f"""
ğŸ“Š ESTRUTURA DOS DADOS:
- Total de registros: {total_registros}
- Total de campos: {len(df.columns)}
"""
            
            if tem_nfe:
                contexto_estrutural += "ğŸ”µ **ContÃ©m NF-e (Nota Fiscal EletrÃ´nica)**\n"
            
            if tem_nfse:
                contexto_estrutural += "ğŸŸ£ **ContÃ©m NFS-e (Nota Fiscal de ServiÃ§o)**\n"
            
            template_usuario = """
{contexto_estrutural}

ğŸ“‹ COLUNAS DISPONÃVEIS:
{resumo_colunas}

ğŸ“ˆ ESTATÃSTICAS:
{estatisticas}

ğŸ“„ REGISTROS COMPLETOS (JSON):
{dados_json}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ” PERGUNTA DO USUÃRIO: {pergunta}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
INSTRUÃ‡Ã•ES CRÃTICAS:
1. Use APENAS os dados JSON acima para responder
2. Cite valores EXATOS dos registros JSON
3. Se a pergunta menciona "prestador", "tomador", "emitente", "destinatÃ¡rio": busque nos campos correspondentes do JSON
4. Para perguntas sobre valores: use as estatÃ­sticas e dados JSON
5. Para perguntas sobre quantidade: use total_registros
6. Responda de forma direta e natural
7. NÃƒO invente dados - use apenas o que estÃ¡ no JSON
8. Se nÃ£o encontrar no JSON, diga claramente que o dado nÃ£o estÃ¡ disponÃ­vel
"""
            
            human_prompt = HumanMessagePromptTemplate.from_template(template_usuario)
            chat_prompt = ChatPromptTemplate.from_messages([sistema_prompt, human_prompt])

            chain = (
                RunnablePassthrough.assign(
                    contexto_estrutural=lambda x: contexto_estrutural,
                    resumo_colunas=lambda x: resumo_colunas,
                    estatisticas=lambda x: estatisticas if estatisticas else "Nenhuma coluna numÃ©rica de valor encontrada",
                    dados_json=lambda x: dados_json,
                    pergunta=lambda x: x["pergunta"]
                )
                | chat_prompt
                | self.llm
                | StrOutputParser()
            )

            resposta = chain.invoke({"pergunta": pergunta})
            resposta = re.sub(r'<[^>]+>', '', resposta)
            resposta = resposta.strip()
            
            if not resposta or len(resposta) < 10:
                resposta = "Desculpe, nÃ£o consegui processar sua pergunta com os dados disponÃ­veis."
            
            logger.info("âœ… Resposta gerada com sucesso")
            return resposta

        except Exception as e:
            logger.error(f"Erro ao responder: {e}")
            return f"Erro ao processar consulta: {str(e)}"

    def _responder_pdf(self, pergunta, contexto_pdf, sistema_prompt):
        """Responde usando APENAS documentos PDF"""
        logger.info("ğŸ“„ Modo: PDF")
        
        try:
            template_usuario = """
ğŸ“„ CONTEXTO DO DOCUMENTO PDF
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{contexto_pdf}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PERGUNTA: {pergunta}

INSTRUÃ‡Ã•ES:
- Use APENAS as informaÃ§Ãµes do documento acima
- Responda de forma direta
- NÃ£o mencione "segundo o PDF" ou "no documento"
- Se nÃ£o encontrar, diga claramente
- Sem tags HTML
"""
            
            human_prompt = HumanMessagePromptTemplate.from_template(template_usuario)
            chat_prompt = ChatPromptTemplate.from_messages([sistema_prompt, human_prompt])

            chain = (
                RunnablePassthrough.assign(
                    contexto_pdf=lambda x: contexto_pdf,
                    pergunta=lambda x: x["pergunta"]
                )
                | chat_prompt
                | self.llm
                | StrOutputParser()
            )

            resposta = chain.invoke({"pergunta": pergunta})
            resposta = re.sub(r'<[^>]+>', '', resposta)
            resposta = resposta.strip()
            
            logger.info("âœ… Resposta PDF gerada")
            return resposta

        except Exception as e:
            logger.error(f"Erro ao responder PDF: {e}")
            return f"Erro ao analisar documento: {str(e)}"

    def _responder_consolidado(self, pergunta, df, contexto_pdf, sistema_prompt):
        """Responde usando DADOS + PDFs juntos"""
        logger.info("ğŸ”€ Modo: Consolidado")
        
        try:
            tem_nfe = any(col.startswith('emit_') for col in df.columns)
            tem_nfse = any(col.startswith('prestador_') for col in df.columns)
            
            contexto_estrutural = f"Total de registros: {len(df)}\n"
            if tem_nfe:
                contexto_estrutural += "ğŸ”µ ContÃ©m NF-e\n"
            if tem_nfse:
                contexto_estrutural += "ğŸŸ£ ContÃ©m NFS-e\n"
            
            colunas_originais = df.columns.tolist()
            colunas_formatadas = [self._formatar_nome_campo(col) for col in colunas_originais[:30]]
            
            dados_json = df.head(30).to_json(orient='records', force_ascii=False, indent=2)
            
            template_usuario = """
ğŸ“Š REGISTROS FISCAIS ELETRÃ”NICOS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{contexto_estrutural}

Campos: {colunas}

Dados JSON:
{dados_json}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“„ DOCUMENTOS FISCAIS (PDF)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{contexto_pdf}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PERGUNTA: {pergunta}

INSTRUÃ‡Ã•ES:
- Analise ambas as fontes
- Responda de forma integrada
- Cite origem apenas se houver discrepÃ¢ncia
- Sem tags HTML
"""
            
            human_prompt = HumanMessagePromptTemplate.from_template(template_usuario)
            chat_prompt = ChatPromptTemplate.from_messages([sistema_prompt, human_prompt])

            chain = (
                RunnablePassthrough.assign(
                    contexto_estrutural=lambda x: contexto_estrutural,
                    colunas=lambda x: ", ".join(colunas_formatadas),
                    dados_json=lambda x: dados_json,
                    contexto_pdf=lambda x: contexto_pdf[:1500],
                    pergunta=lambda x: x["pergunta"]
                )
                | chat_prompt
                | self.llm
                | StrOutputParser()
            )

            resposta = chain.invoke({"pergunta": pergunta})
            resposta = re.sub(r'<[^>]+>', '', resposta)
            resposta = resposta.strip()
            
            logger.info("âœ… Resposta consolidada gerada")
            return resposta

        except Exception as e:
            logger.error(f"Erro ao responder consolidado: {e}")
            return f"Erro ao analisar dados consolidados: {str(e)}"


# âœ¨ INSTÃ‚NCIA GLOBAL COM MEMÃ“RIA
try:
    llm_inteligente = LLMInteligente()
    logger.info("âœ… InstÃ¢ncia LLMInteligente criada com memÃ³ria")
except Exception as e:
    logger.error(f"âŒ Falha ao inicializar LLM: {e}")
    llm_inteligente = None


def gerar_resposta_llm(pergunta, df=None, contexto_pdf=None, historico=None):
    """
    Wrapper compatÃ­vel - ğŸ§  AGORA COM MEMÃ“RIA AUTOMÃTICA
    O parÃ¢metro 'historico' nÃ£o Ã© mais necessÃ¡rio (mantido por compatibilidade)
    """
    if llm_inteligente is None:
        return "Erro: LLM nÃ£o foi inicializado"

    # O histÃ³rico agora Ã© gerenciado automaticamente pela MemoriaInteligente
    return llm_inteligente.gerar_resposta_llm(pergunta, df, contexto_pdf)
